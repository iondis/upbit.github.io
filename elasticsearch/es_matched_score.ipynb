{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "class JsonDict(dict):\n",
    "    \"\"\"general json object that allows attributes to be bound to and also behaves like a dict\"\"\"\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        try:\n",
    "            return self[attr]\n",
    "        except KeyError:\n",
    "            raise AttributeError(r\"'JsonDict' object has no attribute '%s'\" % attr)\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "\n",
    "def parse_json(json_str):\n",
    "    \"\"\"parse str into JsonDict\"\"\"\n",
    "\n",
    "    def _obj_hook(pairs):\n",
    "        \"\"\"convert json object to python object\"\"\"\n",
    "        o = JsonDict()\n",
    "        for k, v in pairs.items():\n",
    "            o[unicode(k)] = v\n",
    "        return o\n",
    "\n",
    "    return json.loads(json_str, object_hook=_obj_hook)\n",
    "\n",
    "class ElasticSearch(dict):\n",
    "    timeout = 2\n",
    "\n",
    "    def __init__(self, base_url='http://100.65.6.44:8080', auth=('es', 'Elastic2@user')):\n",
    "        self.base_url = base_url\n",
    "        self.auth = auth\n",
    "\n",
    "    def _search(self, index, doc_type, query_dsl):\n",
    "        url = \"%s/%s/%s/_search?pretty\" % (self.base_url, index, doc_type)\n",
    "\n",
    "        r = requests.post(url, data=query_dsl, auth=self.auth, timeout=self.timeout)\n",
    "        if not r.status_code in [200]:\n",
    "            raise Exception(\"[HTTP %d] %s\\n%s\" % (r.status_code, r.text, r.url))\n",
    "\n",
    "        try:\n",
    "            json_results = parse_json(r.text)\n",
    "            return json_results.hits.hits\n",
    "\n",
    "        except Exception, e:\n",
    "            raise Exception(\"ElasticSearch() response failed: %s\\n%s\" % (e, r.text))\n",
    "\n",
    "    def _doc(self, index, doc_type, doc_id):\n",
    "        url = \"%s/%s/%s/%s?pretty\" % (self.base_url, index, doc_type, doc_id)\n",
    "\n",
    "        r = requests.get(url, auth=self.auth, timeout=self.timeout)\n",
    "        if not r.status_code in [200]:\n",
    "            raise Exception(\"[HTTP %d] %s\\n%s\" % (r.status_code, r.text, r.url))\n",
    "\n",
    "        try:\n",
    "            json_results = parse_json(r.text)\n",
    "            return json_results._source\n",
    "\n",
    "        except Exception, e:\n",
    "            raise Exception(\"ElasticSearch() response failed: %s\\n%s\" % (e, r.text))\n",
    "\n",
    "    def _analyze(self, index, text, analyzer='ik_smart'):\n",
    "        url = \"%s/%s/_analyze?pretty\" % (self.base_url, index)\n",
    "        \n",
    "        params = {\n",
    "            'text': text,\n",
    "            'analyzer': analyzer,\n",
    "        }\n",
    "\n",
    "        r = requests.get(url, params=params, auth=self.auth, timeout=self.timeout)\n",
    "        if not r.status_code in [200]:\n",
    "            raise Exception(\"[HTTP %d] %s\\n%s\" % (r.status_code, r.text, r.url))\n",
    "\n",
    "        try:\n",
    "            json_results = parse_json(r.text)\n",
    "            return json_results.tokens\n",
    "\n",
    "        except Exception, e:\n",
    "            raise Exception(\"ElasticSearch() response failed: %s\\n%s\" % (e, r.text))\n",
    "\n",
    "    def _termvectors(self, index, doc_type, doc_id, fields=['title'], field_statistics=True, term_statistics=True):\n",
    "        url = \"%s/%s/%s/%s/_termvectors?pretty\" % (self.base_url, index, doc_type, doc_id)\n",
    "\n",
    "        params = {\n",
    "            'fields': ','.join(fields),\n",
    "            'field_statistics': field_statistics,\n",
    "            'term_statistics': term_statistics,\n",
    "            'offsets': True,\n",
    "            'payloads': True,\n",
    "            'positions': True,\n",
    "        }\n",
    "\n",
    "        r = requests.get(url, params=params, auth=self.auth, timeout=self.timeout)\n",
    "        if not r.status_code in [200]:\n",
    "            raise Exception(\"[HTTP %d] %s\\n%s\" % (r.status_code, r.text, r.url))\n",
    "\n",
    "        try:\n",
    "            json_results = parse_json(r.text)\n",
    "            return json_results.term_vectors\n",
    "\n",
    "        except Exception, e:\n",
    "            raise Exception(\"ElasticSearch() response failed: %s\\n%s\" % (e, r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查询doc的分词和TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当/(145,7.1972) 幸福/(345,6.3329) 来/(328,6.3834) 敲门/(25,8.9347) \n",
      "当/(148,7.1744) 真爱/(86,7.7143) 来/(309,6.4405) 敲门/(26,8.8940) \n",
      "幸福/(345,6.3329) 来/(328,6.3834) 敲门/(25,8.9347) 2017/(666,5.6760) \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_doc_terms(index, doc_id, field='title'):\n",
    "    if index == 'media_search':\n",
    "        es = ElasticSearch('http://100.66.1.11:8080')\n",
    "    else:\n",
    "        es = ElasticSearch('http://100.65.6.44:8080')\n",
    "\n",
    "    termvectors = es._termvectors(index, 'doc', doc_id)\n",
    "    tmp_vectors = [ (v.tokens[0].position, t, v.doc_freq) for t,v in termvectors[field].terms.items() ]\n",
    "    max_doc_count = termvectors[field].field_statistics.doc_count\n",
    "    return [ v[1:] for v in sorted(tmp_vectors) ], max_doc_count\n",
    "\n",
    "doc_list = [\n",
    "    ('media_search','14176543619570405528'),\n",
    "    ('media_search','12994864364961195086'),\n",
    "    ('media_search','11217959712449608799'),\n",
    "]\n",
    "\n",
    "d = 0.8\n",
    "dt = []\n",
    "for index,doc_id in doc_list:\n",
    "    termvectors, max_doc_count = get_doc_terms(index, doc_id)\n",
    "    dt.append([ v[0] for v in termvectors ])\n",
    "    for term,tf in termvectors:\n",
    "        popular = math.log(1 + (max_doc_count - d*tf + 0.5) / (d*tf + 0.5))\n",
    "        print \"%s/(%d,%.4f)\" % (term, tf, popular),\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算query的匹配分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当 幸福 来 敲门\n"
     ]
    }
   ],
   "source": [
    "def get_query_terms(query):\n",
    "    es = ElasticSearch('http://100.66.1.11:8080')\n",
    "    tokens = es._analyze('media_search', query)\n",
    "    return [ t.token for t in tokens ]\n",
    "\n",
    "query = u\"当幸福来敲门\"\n",
    "qt = get_query_terms(query)\n",
    "for term in qt:\n",
    "    print term, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当 幸福 来 敲门 >>> 4.0 True\n",
      "当 来 敲门 >>> 2.25 False\n",
      "幸福 来 敲门 >>> 2.25 False\n"
     ]
    }
   ],
   "source": [
    "def get_matched_terms(doc_terms, query_terms):\n",
    "    result = []\n",
    "    for term in query_terms:\n",
    "        if term in doc_terms:\n",
    "            result.append(term)\n",
    "    return result\n",
    "\n",
    "def calc_matched_score(query_terms, matched_terms, term_weights={}):\n",
    "    weights = 0.0\n",
    "    for term in matched_terms:\n",
    "        weights += term_weights.get(term, 1.0)\n",
    "    coord = weights * len(matched_terms) / len(query_terms)\n",
    "    return coord\n",
    "\n",
    "# coord是匹配程度分值，和doc的terms个数相同则表示全匹配\n",
    "mt = [ [] for i in range(len(doc_list)) ]\n",
    "coord = [ 0 for i in range(len(doc_list)) ]\n",
    "for idx,(_,doc_id) in enumerate(doc_list):\n",
    "    mt[idx] = get_matched_terms(dt[idx], qt)\n",
    "    coord[idx] = calc_matched_score(qt, mt[idx])\n",
    "    print ' '.join(mt[idx]), \">>>\", coord[idx], coord[idx] == len(dt[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算正向匹配的最大term数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三生/(2786,8.2006) 三世/(2007,8.5285) 爆笑/(24881,6.0113) 花絮/(28915,5.8611) 杨/(35697,5.6504) 幂/(5964,7.4396) 一言不合/(1816,8.6285) 飙车/(1918,8.5738) 赵/(23899,6.0516) 廷/(2047,8.5088) 害羞/(1129,9.1036) \n",
      "三生/(2864,8.1693) 三世/(2065,8.4963) 十里/(1622,8.7377) 桃花/(2783,8.1980) 花絮/(29023,5.8537) 赵/(23363,6.0706) 廷/(2087,8.4857) 爆笑/(24374,6.0282) 变/(19645,6.2439) 装/(14031,6.5805) 逗笑/(419,10.0902) 杨/(35305,5.6577) 幂/(6066,7.4190) \n",
      "爆笑 花絮 杨 幂 >>> 4.0 False\n",
      "爆笑 花絮 杨 幂 >>> 4.0 False\n"
     ]
    }
   ],
   "source": [
    "doc_list = [\n",
    "    ('mugc_search','940238209026957285'),\n",
    "    # ('mugc_search','15709473540772892745'),\n",
    "    # ('mugc_search','18108452449665614989'),\n",
    "    ('mugc_search','4009975089580785448'),\n",
    "]\n",
    "\n",
    "d = 0.8\n",
    "dt = []\n",
    "for index,doc_id in doc_list:\n",
    "    termvectors, max_doc_count = get_doc_terms(index, doc_id)\n",
    "    dt.append([ v[0] for v in termvectors ])\n",
    "    for term,tf in termvectors:\n",
    "        popular = math.log(1 + (max_doc_count - d*tf + 0.5) / (d*tf + 0.5))\n",
    "        print \"%s/(%d,%.4f)\" % (term, tf, popular),\n",
    "    print \"\"\n",
    "\n",
    "# coord是匹配程度分值，和doc的terms个数相同则表示全匹配\n",
    "mt = [ [] for i in range(len(doc_list)) ]\n",
    "coord = [ 0 for i in range(len(doc_list)) ]\n",
    "for idx,(_,doc_id) in enumerate(doc_list):\n",
    "    mt[idx] = get_matched_terms(dt[idx], qt)\n",
    "    coord[idx] = calc_matched_score(qt, mt[idx])\n",
    "    print ' '.join(mt[idx]), \">>>\", coord[idx], coord[idx] == len(dt[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爆笑 花絮 杨 幂 \n",
      "4 三生 三世 爆笑 花絮 杨 幂 一言不合 飙车 赵 廷 害羞\n",
      "3 三生 三世 十里 桃花 花絮 赵 廷 爆笑 变 装 逗笑 杨 幂\n"
     ]
    }
   ],
   "source": [
    "def longest_increasing_subsequence(array):\n",
    "    tmp_len = len(array);\n",
    "    LIS = [1 for i in range(tmp_len)];\n",
    "    for i in range(len(array)):\n",
    "        j=0;\n",
    "        while j <= i:\n",
    "            if (array[i] > array[j]) and (LIS[i] < LIS[j]+1):\n",
    "                LIS[i] = LIS[j]+1;\n",
    "            j = j+1;\n",
    "    return max(LIS);\n",
    "\n",
    "def max_LIS(doc_terms, query_terms):\n",
    "    seq_terms = [ (term, doc_terms.index(term)) for term in query_terms if term in doc_terms ]\n",
    "    # 取出序号转换为求最长递增子序列(LIS)问题\n",
    "    seq_list = [s[1] for s in seq_terms]\n",
    "    return seq_list\n",
    "\n",
    "query = u\"爆笑花絮杨幂\"\n",
    "qt = get_query_terms(query)\n",
    "for term in qt:\n",
    "    print term, \n",
    "print \"\"\n",
    "\n",
    "st = [ [] for i in range(len(doc_list)) ]\n",
    "for idx in range(len(doc_list)):\n",
    "    st[idx] = max_LIS(dt[idx], qt)\n",
    "    print longest_increasing_subsequence(st[idx]), \" \".join(dt[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
